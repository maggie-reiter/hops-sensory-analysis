{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Keywords and phrases extraction\n",
    "This notebook analyzes key words, phrases, and linguistic patterns from cleaned sensory comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package versions:\n",
      "Python: 3.12.7 (tags/v3.12.7:0b05ead, Oct  1 2024, 03:06:41) [MSC v.1941 64 bit (AMD64)]\n",
      "pandas: 2.2.3\n",
      "nltk: 3.9.1\n",
      "spacy: 3.8.4\n",
      "Downloading NLTK wordnet...\n",
      "Loaded spaCy model: en_core_web_sm\n"
     ]
    }
   ],
   "source": [
    "# Import required packages\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "print(\"Package versions:\")\n",
    "print(f\"Python: {__import__('sys').version}\")\n",
    "print(f\"pandas: {pd.__version__}\")\n",
    "print(f\"nltk: {nltk.__version__}\")\n",
    "print(f\"spacy: {spacy.__version__}\")\n",
    "\n",
    "# Download required NLTK data if not already present\n",
    "try:\n",
    "    nltk.data.find(\"corpora/stopwords\")\n",
    "except LookupError:\n",
    "    print(\"Downloading NLTK stopwords...\")\n",
    "    nltk.download(\"stopwords\", quiet=True)\n",
    "\n",
    "try:\n",
    "    nltk.data.find(\"corpora/wordnet\")\n",
    "except LookupError:\n",
    "    print(\"Downloading NLTK wordnet...\")\n",
    "    nltk.download(\"wordnet\", quiet=True)\n",
    "\n",
    "# Load spacy model\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    print(f\"Loaded spaCy model: en_core_web_sm\")\n",
    "except OSError:\n",
    "    print(\"Error: spaCy model 'en_core_web_sm' not found. Please install with:\")\n",
    "    print(\"python -m spacy download en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "Load the cleaned sensory comments data from the previous notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 268 cleaned comments\n"
     ]
    }
   ],
   "source": [
    "# Load cleaned comments data\n",
    "with open(\"comments_cleaned.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(data)} cleaned comments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random sample of comments:\n",
      "Crisp\n",
      "Better as it warms up \n",
      "Dank, mango, bitter, citrus\n",
      "Delightful citrus fruity tropical\n",
      "Cowabunga! Dole pineapple juice and mango nectar. Thiol forward with healthy undertones of sweetened lime juice/Newmans Own limeade. Not incredibly complex, but it doesnâ€™t need to be. Well balanced, excellent intensity.\n",
      "Mild flavor overall. Sample is slightly sweet, trace tropical fruit, low to moderate bitterness, with a trace of resiny finish. \n",
      "Pleasant rose/rose hop flavor aroma with a juicy finish, pineapple, melon, cherry hint of an herbal/grassy finish but nice\n",
      "Pleasant aroma\n",
      "Touch of herb to make it natural and not artificial\n",
      "Clove, banana\n"
     ]
    }
   ],
   "source": [
    "# Display a random sample of comments\n",
    "print(\"Random sample of comments:\")\n",
    "\n",
    "sample_comments = random.sample(data, 10)\n",
    "for i, comment in enumerate(sample_comments, 1):\n",
    "    print(f\"{comment['text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample distribution:\n",
      "  Dried hops comments: 181 (67.5%)\n",
      "  Beer comments:        87 (32.5%)\n",
      "  Total comments:      268\n"
     ]
    }
   ],
   "source": [
    "# Separate comments by sample type\n",
    "hops_texts = [item[\"text\"] for item in data if item[\"sample\"] == \"Dried Hops\"]\n",
    "beer_texts = [item[\"text\"] for item in data if item[\"sample\"] == \"Beer\"]\n",
    "\n",
    "print(f\"Sample distribution:\")\n",
    "print(\n",
    "    f\"  Dried hops comments: {len(hops_texts):3d} ({len(hops_texts)/len(data)*100:.1f}%)\"\n",
    ")\n",
    "print(\n",
    "    f\"  Beer comments:       {len(beer_texts):3d} ({len(beer_texts)/len(data)*100:.1f}%)\"\n",
    ")\n",
    "print(f\"  Total comments:      {len(data):3d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-gram frequency analysis\n",
    "Analyze the most frequently occurring single words (unigrams), word pairs (bigrams), and three-word combinations (trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up text preprocessing components\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Keep some negation and intensity words that are important for sensory analysis\n",
    "important_words = {\"not\", \"no\", \"very\", \"never\"}\n",
    "stop_words -= important_words\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dried hops n-gram analysis:\n",
      "\n",
      "Top 10 unigrams:\n",
      "   1. fruit           (29)\n",
      "   2. tropical        (27)\n",
      "   3. citrus          (25)\n",
      "   4. peach           (16)\n",
      "   5. garlic          (15)\n",
      "   6. onion           (15)\n",
      "   7. mango           (14)\n",
      "   8. pineapple       (13)\n",
      "   9. fruity          (11)\n",
      "  10. bright          (11)\n",
      "\n",
      "Top 10 bigrams:\n",
      "   1. onion garlic         (11)\n",
      "   2. stone fruit          ( 7)\n",
      "   3. passion fruit        ( 4)\n",
      "   4. tropical stone       ( 3)\n",
      "   5. orange zest          ( 3)\n",
      "   6. pale ale             ( 3)\n",
      "   7. tropical fruit       ( 3)\n",
      "   8. slight onion         ( 3)\n",
      "   9. smell like           ( 3)\n",
      "  10. very nice            ( 3)\n",
      "\n",
      "Top trigrams:\n",
      "   1. tropical stone fruit      ( 3)\n",
      "   2. slight onion garlic       ( 3)\n",
      "   3. mix floral citrus         ( 2)\n",
      "   4. india pale ale            ( 2)\n",
      "   5. melon stone fruit         ( 2)\n"
     ]
    }
   ],
   "source": [
    "# Process dried hops comments for n-gram extraction\n",
    "hops_words = []\n",
    "hops_bigrams = []\n",
    "hops_trigrams = []\n",
    "\n",
    "for text in hops_texts:\n",
    "    # Clean and preprocess text\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Remove punctuation\n",
    "    text = re.sub(r\"\\d+\", \"\", text)  # Remove digits\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Filter and lemmatize tokens\n",
    "    cleaned_tokens = [\n",
    "        lemmatizer.lemmatize(tok)\n",
    "        for tok in tokens\n",
    "        if tok not in stop_words and len(tok) > 2\n",
    "    ]\n",
    "\n",
    "    # Collect unigrams\n",
    "    hops_words.extend(cleaned_tokens)\n",
    "\n",
    "    # Generate bigrams\n",
    "    for i in range(len(cleaned_tokens) - 1):\n",
    "        hops_bigrams.append(f\"{cleaned_tokens[i]} {cleaned_tokens[i+1]}\")\n",
    "\n",
    "    # Generate trigrams\n",
    "    for i in range(len(cleaned_tokens) - 2):\n",
    "        hops_trigrams.append(\n",
    "            f\"{cleaned_tokens[i]} {cleaned_tokens[i+1]} {cleaned_tokens[i+2]}\"\n",
    "        )\n",
    "\n",
    "# Count n-grams\n",
    "hops_unigram_counts = Counter(hops_words)\n",
    "hops_bigram_counts = Counter(hops_bigrams)\n",
    "hops_trigram_counts = Counter(hops_trigrams)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nDried hops n-gram analysis:\")\n",
    "\n",
    "print(f\"\\nTop 10 unigrams:\")\n",
    "for i, (word, count) in enumerate(hops_unigram_counts.most_common(10), 1):\n",
    "    print(f\"  {i:2d}. {word:<15} ({count:2d})\")\n",
    "\n",
    "print(f\"\\nTop 10 bigrams:\")\n",
    "for i, (bigram, count) in enumerate(hops_bigram_counts.most_common(10), 1):\n",
    "    print(f\"  {i:2d}. {bigram:<20} ({count:2d})\")\n",
    "\n",
    "# Filter trigrams that occur more than once\n",
    "frequent_trigrams = [(tg, c) for tg, c in hops_trigram_counts.items() if c > 1]\n",
    "frequent_trigrams.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"\\nTop trigrams:\")\n",
    "for i, (trigram, count) in enumerate(frequent_trigrams[:10], 1):\n",
    "    print(f\"  {i:2d}. {trigram:<25} ({count:2d})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beer n-gram analysis:\n",
      "\n",
      "Top 10 unigrams:\n",
      "   1. fruit           (23)\n",
      "   2. aroma           (21)\n",
      "   3. flavor          (15)\n",
      "   4. hop             (15)\n",
      "   5. citrus          (11)\n",
      "   6. not             (10)\n",
      "   7. sweet           (10)\n",
      "   8. pleasant        (10)\n",
      "   9. bitterness      (10)\n",
      "  10. tropical        ( 9)\n",
      "\n",
      "Top 10 bigrams:\n",
      "   1. stone fruit          ( 5)\n",
      "   2. hop flavor           ( 5)\n",
      "   3. sweet fruit          ( 4)\n",
      "   4. pineapple melon      ( 3)\n",
      "   5. onion garlic         ( 3)\n",
      "   6. tropical fruit       ( 3)\n",
      "   7. hop aroma            ( 3)\n",
      "   8. passion fruit        ( 2)\n",
      "   9. aroma citrus         ( 2)\n",
      "  10. citrus stone         ( 2)\n",
      "\n",
      "Top trigrams:\n",
      "   1. aroma citrus stone        ( 2)\n",
      "   2. citrus stone fruit        ( 2)\n",
      "   3. not much hop              ( 2)\n",
      "   4. overall hop aroma         ( 2)\n",
      "   5. hop aroma intensity       ( 2)\n",
      "   6. slight onion garlic       ( 2)\n"
     ]
    }
   ],
   "source": [
    "# Process beer comments for n-gram extraction\n",
    "beer_words = []\n",
    "beer_bigrams = []\n",
    "beer_trigrams = []\n",
    "\n",
    "for text in beer_texts:\n",
    "    # Clean and preprocess text\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Remove punctuation\n",
    "    text = re.sub(r\"\\d+\", \"\", text)  # Remove digits\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Filter and lemmatize tokens\n",
    "    cleaned_tokens = [\n",
    "        lemmatizer.lemmatize(tok)\n",
    "        for tok in tokens\n",
    "        if tok not in stop_words and len(tok) > 2\n",
    "    ]\n",
    "\n",
    "    # Collect unigrams\n",
    "    beer_words.extend(cleaned_tokens)\n",
    "\n",
    "    # Generate bigrams\n",
    "    for i in range(len(cleaned_tokens) - 1):\n",
    "        beer_bigrams.append(f\"{cleaned_tokens[i]} {cleaned_tokens[i+1]}\")\n",
    "\n",
    "    # Generate trigrams\n",
    "    for i in range(len(cleaned_tokens) - 2):\n",
    "        beer_trigrams.append(\n",
    "            f\"{cleaned_tokens[i]} {cleaned_tokens[i+1]} {cleaned_tokens[i+2]}\"\n",
    "        )\n",
    "\n",
    "# Count n-grams\n",
    "beer_unigram_counts = Counter(beer_words)\n",
    "beer_bigram_counts = Counter(beer_bigrams)\n",
    "beer_trigram_counts = Counter(beer_trigrams)\n",
    "\n",
    "# Display results\n",
    "print(\"Beer n-gram analysis:\")\n",
    "\n",
    "print(f\"\\nTop 10 unigrams:\")\n",
    "for i, (word, count) in enumerate(beer_unigram_counts.most_common(10), 1):\n",
    "    print(f\"  {i:2d}. {word:<15} ({count:2d})\")\n",
    "\n",
    "print(f\"\\nTop 10 bigrams:\")\n",
    "for i, (bigram, count) in enumerate(beer_bigram_counts.most_common(10), 1):\n",
    "    print(f\"  {i:2d}. {bigram:<20} ({count:2d})\")\n",
    "\n",
    "# Filter trigrams that occur more than once\n",
    "frequent_trigrams = [(tg, c) for tg, c in beer_trigram_counts.items() if c > 1]\n",
    "frequent_trigrams.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"\\nTop trigrams:\")\n",
    "for i, (trigram, count) in enumerate(frequent_trigrams[:10], 1):\n",
    "    print(f\"  {i:2d}. {trigram:<25} ({count:2d})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-of-speech analysis\n",
    "Extract and analyze adjectives and adverbs using part-of-speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example POS analysis:\n",
      "Comment #90 [Dried Hops]: \"floral, almost tea like delicate aromatics.\"\n",
      "\n",
      "Adjectives found (2):\n",
      "  â€¢ floral       (POS: ADJ, dependency: amod)\n",
      "  â€¢ delicate     (POS: ADJ, dependency: amod)\n",
      "\n",
      "Adverbs found (1):\n",
      "  â€¢ almost       (POS: ADV, dependency: advmod)\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate part-of-speech tagging with an example comment\n",
    "example_idx = 90\n",
    "example_comment = data[example_idx][\"text\"]\n",
    "sample_type = data[example_idx][\"sample\"]\n",
    "\n",
    "print(f\"Example POS analysis:\")\n",
    "print(f'Comment #{example_idx} [{sample_type}]: \"{example_comment}\"')\n",
    "\n",
    "# Process with spaCy\n",
    "doc = nlp(example_comment)\n",
    "\n",
    "# Extract adjectives\n",
    "adjectives = [\n",
    "    (token.text, token.pos_, token.dep_) for token in doc if token.pos_ == \"ADJ\"\n",
    "]\n",
    "adverbs = [(token.text, token.pos_, token.dep_) for token in doc if token.pos_ == \"ADV\"]\n",
    "\n",
    "print(f\"\\nAdjectives found ({len(adjectives)}):\")\n",
    "if adjectives:\n",
    "    for word, pos, dep in adjectives:\n",
    "        print(f\"  â€¢ {word:<12} (POS: {pos}, dependency: {dep})\")\n",
    "else:\n",
    "    print(\"  (none found)\")\n",
    "\n",
    "print(f\"\\nAdverbs found ({len(adverbs)}):\")\n",
    "if adverbs:\n",
    "    for word, pos, dep in adverbs:\n",
    "        print(f\"  â€¢ {word:<12} (POS: {pos}, dependency: {dep})\")\n",
    "else:\n",
    "    print(\"  (none found)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dried hops POS analysis:\n",
      "\n",
      "Top 10 adjectives:\n",
      "   1. tropical        (25)\n",
      "   2. bright          (11)\n",
      "   3. nice            (10)\n",
      "   4. slight          (10)\n",
      "   5. sweet           ( 9)\n",
      "   6. ripe            ( 5)\n",
      "   7. floral          ( 5)\n",
      "   8. light           ( 5)\n",
      "   9. clean           ( 4)\n",
      "  10. good            ( 4)\n",
      "\n",
      "Top 10 adverbs:\n",
      "   1. very            (10)\n",
      "   2. too             ( 4)\n",
      "   3. well            ( 3)\n",
      "   4. really          ( 3)\n",
      "   5. so              ( 2)\n",
      "   6. forward         ( 2)\n",
      "   7. almost          ( 2)\n",
      "   8. incredibly      ( 1)\n",
      "   9. there           ( 1)\n",
      "  10. as              ( 1)\n"
     ]
    }
   ],
   "source": [
    "# Extract adjectives and adverbs from dried hops comments\n",
    "hops_adjectives = []\n",
    "hops_adverbs = []\n",
    "\n",
    "for text in hops_texts:\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        if token.pos_ == \"ADJ\":\n",
    "            hops_adjectives.append(token.lemma_.lower())\n",
    "        elif token.pos_ == \"ADV\":\n",
    "            hops_adverbs.append(token.lemma_.lower())\n",
    "\n",
    "hops_adj_counts = Counter(hops_adjectives)\n",
    "hops_adv_counts = Counter(hops_adverbs)\n",
    "\n",
    "# Display results\n",
    "print(\"Dried hops POS analysis:\")\n",
    "\n",
    "print(f\"\\nTop 10 adjectives:\")\n",
    "for i, (adj, count) in enumerate(hops_adj_counts.most_common(10), 1):\n",
    "    print(f\"  {i:2d}. {adj:<15} ({count:2d})\")\n",
    "\n",
    "print(f\"\\nTop 10 adverbs:\")\n",
    "for i, (adv, count) in enumerate(hops_adv_counts.most_common(10), 1):\n",
    "    print(f\"  {i:2d}. {adv:<15} ({count:2d})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beer POS analysis:\n",
      "\n",
      "Top 10 adjectives:\n",
      "   1. sweet           (10)\n",
      "   2. pleasant        (10)\n",
      "   3. tropical        ( 9)\n",
      "   4. slight          ( 9)\n",
      "   5. clean           ( 8)\n",
      "   6. mild            ( 7)\n",
      "   7. low             ( 7)\n",
      "   8. nice            ( 7)\n",
      "   9. strong          ( 5)\n",
      "  10. catty           ( 4)\n",
      "\n",
      "Top 10 adverbs:\n",
      "   1. slightly        ( 7)\n",
      "   2. forward         ( 5)\n",
      "   3. very            ( 4)\n",
      "   4. well            ( 3)\n",
      "   5. really          ( 3)\n",
      "   6. kind            ( 2)\n",
      "   7. of              ( 2)\n",
      "   8. quite           ( 2)\n",
      "   9. too             ( 2)\n",
      "  10. maybe           ( 2)\n"
     ]
    }
   ],
   "source": [
    "# Extract adjectives and adverbs from beer comments\n",
    "beer_adjectives = []\n",
    "beer_adverbs = []\n",
    "\n",
    "for text in beer_texts:\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        if token.pos_ == \"ADJ\":\n",
    "            beer_adjectives.append(token.lemma_.lower())\n",
    "        elif token.pos_ == \"ADV\":\n",
    "            beer_adverbs.append(token.lemma_.lower())\n",
    "\n",
    "beer_adj_counts = Counter(beer_adjectives)\n",
    "beer_adv_counts = Counter(beer_adverbs)\n",
    "\n",
    "# Display results\n",
    "print(\"Beer POS analysis:\")\n",
    "\n",
    "print(f\"\\nTop 10 adjectives:\")\n",
    "for i, (adj, count) in enumerate(beer_adj_counts.most_common(10), 1):\n",
    "    print(f\"  {i:2d}. {adj:<15} ({count:2d})\")\n",
    "\n",
    "print(f\"\\nTop 10 adverbs:\")\n",
    "for i, (adv, count) in enumerate(beer_adv_counts.most_common(10), 1):\n",
    "    print(f\"  {i:2d}. {adv:<15} ({count:2d})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Onion garlic descriptor analysis\n",
    "Analyze the frequency of onion/garlic mentions, a common off-flavor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OG descriptor analysis:\n",
      "\n",
      "Dried hops comments with onion/garlic: 18/181 (9.9%)\n",
      "Beer comments with onion/garlic: 5/87 (5.7%)\n"
     ]
    }
   ],
   "source": [
    "# Count onion/garlic mentions\n",
    "onion_garlic_pattern = r\"\\b(onion|garlic)\\b\"\n",
    "\n",
    "hops_onion_garlic = sum(\n",
    "    1 for txt in hops_texts if re.search(onion_garlic_pattern, txt, flags=re.I)\n",
    ")\n",
    "\n",
    "beer_onion_garlic = sum(\n",
    "    1 for txt in beer_texts if re.search(onion_garlic_pattern, txt, flags=re.I)\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"OG descriptor analysis:\")\n",
    "\n",
    "print(\n",
    "    f\"\\nDried hops comments with onion/garlic: {hops_onion_garlic}/{len(hops_texts)} ({hops_onion_garlic / len(hops_texts) * 100:.1f}%)\"\n",
    ")\n",
    "print(\n",
    "    f\"Beer comments with onion/garlic: {beer_onion_garlic}/{len(beer_texts)} ({beer_onion_garlic / len(beer_texts) * 100:.1f}%)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical descriptor analysis\n",
    "Define different descriptor categories (tropical fruits, citrus fruits, etc.) and analyze their prevalence across all comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical descriptor analysis:\n",
      "\n",
      "Dried hops\n",
      "Rank Category           Count   Percentage\n",
      "---------------------------------------------\n",
      "1    tropical fruits    48         26.5%\n",
      "2    citrus fruits      45         24.9%\n",
      "3    fruit (general)    39         21.5%\n",
      "4    stone fruits       23         12.7%\n",
      "5    onion garlic       18          9.9%\n",
      "6    melons             8           4.4%\n",
      "7    berries            7           3.9%\n",
      "\n",
      "Beer\n",
      "Rank Category           Count   Percentage\n",
      "---------------------------------------------\n",
      "1    tropical fruits    21         24.1%\n",
      "2    fruit (general)    21         24.1%\n",
      "3    citrus fruits      18         20.7%\n",
      "4    stone fruits       9          10.3%\n",
      "5    onion garlic       5           5.7%\n",
      "6    melons             5           5.7%\n",
      "7    berries            0           0.0%\n"
     ]
    }
   ],
   "source": [
    "# Define descriptor categories with regex patterns\n",
    "descriptor_categories = {\n",
    "    \"tropical fruits\": r\"\\b(tropical|pineapple|mango|passion|papaya|guava|lychee)\\b\",\n",
    "    \"citrus fruits\": r\"\\b(citrus|orange|grapefruit|lemon|lime|tangerine|mandarin|yuzu|tangelo)\\b\",\n",
    "    \"stone fruits\": r\"\\b(stone fruit|peach|nectarine|apricot|plum)\\b\",\n",
    "    \"fruit (general)\": r\"\\b(fruit|fruity)\\b\",\n",
    "    \"onion garlic\": r\"\\b(onion|garlic)\\b\",\n",
    "    \"melons\": r\"\\b(melon|watermelon|cantaloupe|honeydew)\\b\",\n",
    "    \"berries\": r\"\\b(berry|strawberry|blackberry|blueberry|raspberry)\\b\",\n",
    "}\n",
    "\n",
    "# Calculate prevalence for dried hops\n",
    "hops_total = len(hops_texts)\n",
    "hops_rows = []\n",
    "for label, pattern in descriptor_categories.items():\n",
    "    n = sum(bool(re.search(pattern, text, flags=re.I)) for text in hops_texts)\n",
    "    hops_rows.append(\n",
    "        {\n",
    "            \"category\": label,\n",
    "            \"n_comments\": n,\n",
    "            \"pct_comments\": n / hops_total,\n",
    "            \"n_total\": hops_total,\n",
    "        }\n",
    "    )\n",
    "hops_prevalence = pd.DataFrame(hops_rows).sort_values(\"pct_comments\", ascending=False)\n",
    "\n",
    "# Calculate prevalence for beer\n",
    "beer_total = len(beer_texts)\n",
    "beer_rows = []\n",
    "for label, pattern in descriptor_categories.items():\n",
    "    n = sum(bool(re.search(pattern, text, flags=re.I)) for text in beer_texts)\n",
    "    beer_rows.append(\n",
    "        {\n",
    "            \"category\": label,\n",
    "            \"n_comments\": n,\n",
    "            \"pct_comments\": n / beer_total,\n",
    "            \"n_total\": beer_total,\n",
    "        }\n",
    "    )\n",
    "beer_prevalence = pd.DataFrame(beer_rows).sort_values(\"pct_comments\", ascending=False)\n",
    "\n",
    "# Display results\n",
    "print(\"Categorical descriptor analysis:\")\n",
    "\n",
    "print(f\"\\nDried hops\")\n",
    "print(f\"{'Rank':<4} {'Category':<18} {'Count':<7} {'Percentage':<10}\")\n",
    "print(\"-\" * 45)\n",
    "for i, row in hops_prevalence.iterrows():\n",
    "    rank = hops_prevalence.index.get_loc(i) + 1\n",
    "    print(\n",
    "        f\"{rank:<4} {row['category']:<18} {row['n_comments']:<7} {row['pct_comments']*100:>7.1f}%\"\n",
    "    )\n",
    "\n",
    "print(f\"\\nBeer\")\n",
    "print(f\"{'Rank':<4} {'Category':<18} {'Count':<7} {'Percentage':<10}\")\n",
    "print(\"-\" * 45)\n",
    "for i, row in beer_prevalence.iterrows():\n",
    "    rank = beer_prevalence.index.get_loc(i) + 1\n",
    "    print(\n",
    "        f\"{rank:<4} {row['category']:<18} {row['n_comments']:<7} {row['pct_comments']*100:>7.1f}%\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
