{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Keywords and phrases extraction\n",
    "This notebook analyzes key words, phrases, and linguistic patterns from cleaned sensory comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package versions:\n",
      "Python: 3.12.7 (tags/v3.12.7:0b05ead, Oct  1 2024, 03:06:41) [MSC v.1941 64 bit (AMD64)]\n",
      "pandas: 2.2.3\n",
      "nltk: 3.9.1\n",
      "spacy: 3.8.4\n",
      "Downloading NLTK wordnet...\n",
      "Loaded spaCy model: en_core_web_sm\n"
     ]
    }
   ],
   "source": [
    "# Import required packages\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "print(\"Package versions:\")\n",
    "print(f\"Python: {__import__('sys').version}\")\n",
    "print(f\"pandas: {pd.__version__}\")\n",
    "print(f\"nltk: {nltk.__version__}\")\n",
    "print(f\"spacy: {spacy.__version__}\")\n",
    "\n",
    "# Download required NLTK data if not already present\n",
    "try:\n",
    "    nltk.data.find(\"corpora/stopwords\")\n",
    "except LookupError:\n",
    "    print(\"Downloading NLTK stopwords...\")\n",
    "    nltk.download(\"stopwords\", quiet=True)\n",
    "\n",
    "try:\n",
    "    nltk.data.find(\"corpora/wordnet\")\n",
    "except LookupError:\n",
    "    print(\"Downloading NLTK wordnet...\")\n",
    "    nltk.download(\"wordnet\", quiet=True)\n",
    "\n",
    "# Load spacy model\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    print(f\"Loaded spaCy model: en_core_web_sm\")\n",
    "except OSError:\n",
    "    print(\"Error: spaCy model 'en_core_web_sm' not found. Please install with:\")\n",
    "    print(\"python -m spacy download en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "Load the cleaned sensory comments data from the previous notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 268 cleaned comments\n"
     ]
    }
   ],
   "source": [
    "# Load cleaned comments data\n",
    "with open(\"comments_cleaned.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(data)} cleaned comments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random sample of comments:\n",
      "Crisp\n",
      "Better as it warms up \n",
      "Dank, mango, bitter, citrus\n",
      "Delightful citrus fruity tropical\n",
      "Cowabunga! Dole pineapple juice and mango nectar. Thiol forward with healthy undertones of sweetened lime juice/Newmans Own limeade. Not incredibly complex, but it doesn’t need to be. Well balanced, excellent intensity.\n",
      "Mild flavor overall. Sample is slightly sweet, trace tropical fruit, low to moderate bitterness, with a trace of resiny finish. \n",
      "Pleasant rose/rose hop flavor aroma with a juicy finish, pineapple, melon, cherry hint of an herbal/grassy finish but nice\n",
      "Pleasant aroma\n",
      "Touch of herb to make it natural and not artificial\n",
      "Clove, banana\n"
     ]
    }
   ],
   "source": [
    "# Display a random sample of comments\n",
    "print(\"Random sample of comments:\")\n",
    "\n",
    "sample_comments = random.sample(data, 10)\n",
    "for i, comment in enumerate(sample_comments, 1):\n",
    "    print(f\"{comment['text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample distribution:\n",
      "  Dried hops comments: 181 (67.5%)\n",
      "  Beer comments:        87 (32.5%)\n",
      "  Total comments:      268\n"
     ]
    }
   ],
   "source": [
    "# Separate comments by sample type\n",
    "hops_texts = [item[\"text\"] for item in data if item[\"sample\"] == \"Dried Hops\"]\n",
    "beer_texts = [item[\"text\"] for item in data if item[\"sample\"] == \"Beer\"]\n",
    "\n",
    "print(f\"Sample distribution:\")\n",
    "print(\n",
    "    f\"  Dried hops comments: {len(hops_texts):3d} ({len(hops_texts)/len(data)*100:.1f}%)\"\n",
    ")\n",
    "print(\n",
    "    f\"  Beer comments:       {len(beer_texts):3d} ({len(beer_texts)/len(data)*100:.1f}%)\"\n",
    ")\n",
    "print(f\"  Total comments:      {len(data):3d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-gram frequency analysis\n",
    "Analyze the most frequently occurring single words (unigrams), word pairs (bigrams), and three-word combinations (trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up text preprocessing components\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Keep some negation and intensity words that are important for sensory analysis\n",
    "important_words = {\"not\", \"no\", \"very\", \"never\"}\n",
    "stop_words -= important_words\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dried hops n-gram analysis:\n",
      "\n",
      "Top 10 unigrams:\n",
      "   1. fruit           (29)\n",
      "   2. tropical        (27)\n",
      "   3. citrus          (25)\n",
      "   4. peach           (16)\n",
      "   5. garlic          (15)\n",
      "   6. onion           (15)\n",
      "   7. mango           (14)\n",
      "   8. pineapple       (13)\n",
      "   9. fruity          (11)\n",
      "  10. bright          (11)\n",
      "\n",
      "Top 10 bigrams:\n",
      "   1. onion garlic         (11)\n",
      "   2. stone fruit          ( 7)\n",
      "   3. passion fruit        ( 4)\n",
      "   4. tropical stone       ( 3)\n",
      "   5. orange zest          ( 3)\n",
      "   6. pale ale             ( 3)\n",
      "   7. tropical fruit       ( 3)\n",
      "   8. slight onion         ( 3)\n",
      "   9. smell like           ( 3)\n",
      "  10. very nice            ( 3)\n",
      "\n",
      "Top trigrams:\n",
      "   1. tropical stone fruit      ( 3)\n",
      "   2. slight onion garlic       ( 3)\n",
      "   3. mix floral citrus         ( 2)\n",
      "   4. india pale ale            ( 2)\n",
      "   5. melon stone fruit         ( 2)\n"
     ]
    }
   ],
   "source": [
    "# Process dried hops comments for n-gram extraction\n",
    "hops_words = []\n",
    "hops_bigrams = []\n",
    "hops_trigrams = []\n",
    "\n",
    "for text in hops_texts:\n",
    "    # Clean and preprocess text\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Remove punctuation\n",
    "    text = re.sub(r\"\\d+\", \"\", text)  # Remove digits\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Filter and lemmatize tokens\n",
    "    cleaned_tokens = [\n",
    "        lemmatizer.lemmatize(tok)\n",
    "        for tok in tokens\n",
    "        if tok not in stop_words and len(tok) > 2\n",
    "    ]\n",
    "\n",
    "    # Collect unigrams\n",
    "    hops_words.extend(cleaned_tokens)\n",
    "\n",
    "    # Generate bigrams\n",
    "    for i in range(len(cleaned_tokens) - 1):\n",
    "        hops_bigrams.append(f\"{cleaned_tokens[i]} {cleaned_tokens[i+1]}\")\n",
    "\n",
    "    # Generate trigrams\n",
    "    for i in range(len(cleaned_tokens) - 2):\n",
    "        hops_trigrams.append(\n",
    "            f\"{cleaned_tokens[i]} {cleaned_tokens[i+1]} {cleaned_tokens[i+2]}\"\n",
    "        )\n",
    "\n",
    "# Count n-grams\n",
    "hops_unigram_counts = Counter(hops_words)\n",
    "hops_bigram_counts = Counter(hops_bigrams)\n",
    "hops_trigram_counts = Counter(hops_trigrams)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nDried hops n-gram analysis:\")\n",
    "\n",
    "print(f\"\\nTop 10 unigrams:\")\n",
    "for i, (word, count) in enumerate(hops_unigram_counts.most_common(10), 1):\n",
    "    print(f\"  {i:2d}. {word:<15} ({count:2d})\")\n",
    "\n",
    "print(f\"\\nTop 10 bigrams:\")\n",
    "for i, (bigram, count) in enumerate(hops_bigram_counts.most_common(10), 1):\n",
    "    print(f\"  {i:2d}. {bigram:<20} ({count:2d})\")\n",
    "\n",
    "# Filter trigrams that occur more than once\n",
    "frequent_trigrams = [(tg, c) for tg, c in hops_trigram_counts.items() if c > 1]\n",
    "frequent_trigrams.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"\\nTop trigrams:\")\n",
    "for i, (trigram, count) in enumerate(frequent_trigrams[:10], 1):\n",
    "    print(f\"  {i:2d}. {trigram:<25} ({count:2d})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beer n-gram analysis:\n",
      "\n",
      "Top 10 unigrams:\n",
      "   1. fruit           (23)\n",
      "   2. aroma           (21)\n",
      "   3. flavor          (15)\n",
      "   4. hop             (15)\n",
      "   5. citrus          (11)\n",
      "   6. not             (10)\n",
      "   7. sweet           (10)\n",
      "   8. pleasant        (10)\n",
      "   9. bitterness      (10)\n",
      "  10. tropical        ( 9)\n",
      "\n",
      "Top 10 bigrams:\n",
      "   1. stone fruit          ( 5)\n",
      "   2. hop flavor           ( 5)\n",
      "   3. sweet fruit          ( 4)\n",
      "   4. pineapple melon      ( 3)\n",
      "   5. onion garlic         ( 3)\n",
      "   6. tropical fruit       ( 3)\n",
      "   7. hop aroma            ( 3)\n",
      "   8. passion fruit        ( 2)\n",
      "   9. aroma citrus         ( 2)\n",
      "  10. citrus stone         ( 2)\n",
      "\n",
      "Top trigrams:\n",
      "   1. aroma citrus stone        ( 2)\n",
      "   2. citrus stone fruit        ( 2)\n",
      "   3. not much hop              ( 2)\n",
      "   4. overall hop aroma         ( 2)\n",
      "   5. hop aroma intensity       ( 2)\n",
      "   6. slight onion garlic       ( 2)\n"
     ]
    }
   ],
   "source": [
    "# Process beer comments for n-gram extraction\n",
    "beer_words = []\n",
    "beer_bigrams = []\n",
    "beer_trigrams = []\n",
    "\n",
    "for text in beer_texts:\n",
    "    # Clean and preprocess text\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Remove punctuation\n",
    "    text = re.sub(r\"\\d+\", \"\", text)  # Remove digits\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Filter and lemmatize tokens\n",
    "    cleaned_tokens = [\n",
    "        lemmatizer.lemmatize(tok)\n",
    "        for tok in tokens\n",
    "        if tok not in stop_words and len(tok) > 2\n",
    "    ]\n",
    "\n",
    "    # Collect unigrams\n",
    "    beer_words.extend(cleaned_tokens)\n",
    "\n",
    "    # Generate bigrams\n",
    "    for i in range(len(cleaned_tokens) - 1):\n",
    "        beer_bigrams.append(f\"{cleaned_tokens[i]} {cleaned_tokens[i+1]}\")\n",
    "\n",
    "    # Generate trigrams\n",
    "    for i in range(len(cleaned_tokens) - 2):\n",
    "        beer_trigrams.append(\n",
    "            f\"{cleaned_tokens[i]} {cleaned_tokens[i+1]} {cleaned_tokens[i+2]}\"\n",
    "        )\n",
    "\n",
    "# Count n-grams\n",
    "beer_unigram_counts = Counter(beer_words)\n",
    "beer_bigram_counts = Counter(beer_bigrams)\n",
    "beer_trigram_counts = Counter(beer_trigrams)\n",
    "\n",
    "# Display results\n",
    "print(\"Beer n-gram analysis:\")\n",
    "\n",
    "print(f\"\\nTop 10 unigrams:\")\n",
    "for i, (word, count) in enumerate(beer_unigram_counts.most_common(10), 1):\n",
    "    print(f\"  {i:2d}. {word:<15} ({count:2d})\")\n",
    "\n",
    "print(f\"\\nTop 10 bigrams:\")\n",
    "for i, (bigram, count) in enumerate(beer_bigram_counts.most_common(10), 1):\n",
    "    print(f\"  {i:2d}. {bigram:<20} ({count:2d})\")\n",
    "\n",
    "# Filter trigrams that occur more than once\n",
    "frequent_trigrams = [(tg, c) for tg, c in beer_trigram_counts.items() if c > 1]\n",
    "frequent_trigrams.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"\\nTop trigrams:\")\n",
    "for i, (trigram, count) in enumerate(frequent_trigrams[:10], 1):\n",
    "    print(f\"  {i:2d}. {trigram:<25} ({count:2d})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-of-speech analysis\n",
    "Extract and analyze adjectives and adverbs using part-of-speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example POS analysis:\n",
      "Comment #90 [Dried Hops]: \"floral, almost tea like delicate aromatics.\"\n",
      "\n",
      "Adjectives found (2):\n",
      "  • floral       (POS: ADJ, dependency: amod)\n",
      "  • delicate     (POS: ADJ, dependency: amod)\n",
      "\n",
      "Adverbs found (1):\n",
      "  • almost       (POS: ADV, dependency: advmod)\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate part-of-speech tagging with an example comment\n",
    "example_idx = 90\n",
    "example_comment = data[example_idx][\"text\"]\n",
    "sample_type = data[example_idx][\"sample\"]\n",
    "\n",
    "print(f\"Example POS analysis:\")\n",
    "print(f'Comment #{example_idx} [{sample_type}]: \"{example_comment}\"')\n",
    "\n",
    "# Process with spaCy\n",
    "doc = nlp(example_comment)\n",
    "\n",
    "# Extract adjectives\n",
    "adjectives = [\n",
    "    (token.text, token.pos_, token.dep_) for token in doc if token.pos_ == \"ADJ\"\n",
    "]\n",
    "adverbs = [(token.text, token.pos_, token.dep_) for token in doc if token.pos_ == \"ADV\"]\n",
    "\n",
    "print(f\"\\nAdjectives found ({len(adjectives)}):\")\n",
    "if adjectives:\n",
    "    for word, pos, dep in adjectives:\n",
    "        print(f\"  • {word:<12} (POS: {pos}, dependency: {dep})\")\n",
    "else:\n",
    "    print(\"  (none found)\")\n",
    "\n",
    "print(f\"\\nAdverbs found ({len(adverbs)}):\")\n",
    "if adverbs:\n",
    "    for word, pos, dep in adverbs:\n",
    "        print(f\"  • {word:<12} (POS: {pos}, dependency: {dep})\")\n",
    "else:\n",
    "    print(\"  (none found)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dried hops POS analysis:\n",
      "\n",
      "Top 10 adjectives:\n",
      "   1. tropical        (25)\n",
      "   2. bright          (11)\n",
      "   3. nice            (10)\n",
      "   4. slight          (10)\n",
      "   5. sweet           ( 9)\n",
      "   6. ripe            ( 5)\n",
      "   7. floral          ( 5)\n",
      "   8. light           ( 5)\n",
      "   9. clean           ( 4)\n",
      "  10. good            ( 4)\n",
      "\n",
      "Top 10 adverbs:\n",
      "   1. very            (10)\n",
      "   2. too             ( 4)\n",
      "   3. well            ( 3)\n",
      "   4. really          ( 3)\n",
      "   5. so              ( 2)\n",
      "   6. forward         ( 2)\n",
      "   7. almost          ( 2)\n",
      "   8. incredibly      ( 1)\n",
      "   9. there           ( 1)\n",
      "  10. as              ( 1)\n"
     ]
    }
   ],
   "source": [
    "# Extract adjectives and adverbs from dried hops comments\n",
    "hops_adjectives = []\n",
    "hops_adverbs = []\n",
    "\n",
    "for text in hops_texts:\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        if token.pos_ == \"ADJ\":\n",
    "            hops_adjectives.append(token.lemma_.lower())\n",
    "        elif token.pos_ == \"ADV\":\n",
    "            hops_adverbs.append(token.lemma_.lower())\n",
    "\n",
    "hops_adj_counts = Counter(hops_adjectives)\n",
    "hops_adv_counts = Counter(hops_adverbs)\n",
    "\n",
    "# Display results\n",
    "print(\"Dried hops POS analysis:\")\n",
    "\n",
    "print(f\"\\nTop 10 adjectives:\")\n",
    "for i, (adj, count) in enumerate(hops_adj_counts.most_common(10), 1):\n",
    "    print(f\"  {i:2d}. {adj:<15} ({count:2d})\")\n",
    "\n",
    "print(f\"\\nTop 10 adverbs:\")\n",
    "for i, (adv, count) in enumerate(hops_adv_counts.most_common(10), 1):\n",
    "    print(f\"  {i:2d}. {adv:<15} ({count:2d})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beer POS analysis:\n",
      "\n",
      "Top 10 adjectives:\n",
      "   1. sweet           (10)\n",
      "   2. pleasant        (10)\n",
      "   3. tropical        ( 9)\n",
      "   4. slight          ( 9)\n",
      "   5. clean           ( 8)\n",
      "   6. mild            ( 7)\n",
      "   7. low             ( 7)\n",
      "   8. nice            ( 7)\n",
      "   9. strong          ( 5)\n",
      "  10. catty           ( 4)\n",
      "\n",
      "Top 10 adverbs:\n",
      "   1. slightly        ( 7)\n",
      "   2. forward         ( 5)\n",
      "   3. very            ( 4)\n",
      "   4. well            ( 3)\n",
      "   5. really          ( 3)\n",
      "   6. kind            ( 2)\n",
      "   7. of              ( 2)\n",
      "   8. quite           ( 2)\n",
      "   9. too             ( 2)\n",
      "  10. maybe           ( 2)\n"
     ]
    }
   ],
   "source": [
    "# Extract adjectives and adverbs from beer comments\n",
    "beer_adjectives = []\n",
    "beer_adverbs = []\n",
    "\n",
    "for text in beer_texts:\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        if token.pos_ == \"ADJ\":\n",
    "            beer_adjectives.append(token.lemma_.lower())\n",
    "        elif token.pos_ == \"ADV\":\n",
    "            beer_adverbs.append(token.lemma_.lower())\n",
    "\n",
    "beer_adj_counts = Counter(beer_adjectives)\n",
    "beer_adv_counts = Counter(beer_adverbs)\n",
    "\n",
    "# Display results\n",
    "print(\"Beer POS analysis:\")\n",
    "\n",
    "print(f\"\\nTop 10 adjectives:\")\n",
    "for i, (adj, count) in enumerate(beer_adj_counts.most_common(10), 1):\n",
    "    print(f\"  {i:2d}. {adj:<15} ({count:2d})\")\n",
    "\n",
    "print(f\"\\nTop 10 adverbs:\")\n",
    "for i, (adv, count) in enumerate(beer_adv_counts.most_common(10), 1):\n",
    "    print(f\"  {i:2d}. {adv:<15} ({count:2d})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Onion garlic descriptor analysis\n",
    "Analyze the frequency of onion/garlic mentions, a common off-flavor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OG descriptor analysis:\n",
      "\n",
      "Dried hops comments with onion/garlic: 18/181 (9.9%)\n",
      "Beer comments with onion/garlic: 5/87 (5.7%)\n"
     ]
    }
   ],
   "source": [
    "# Count onion/garlic mentions\n",
    "onion_garlic_pattern = r\"\\b(onion|garlic)\\b\"\n",
    "\n",
    "hops_onion_garlic = sum(\n",
    "    1 for txt in hops_texts if re.search(onion_garlic_pattern, txt, flags=re.I)\n",
    ")\n",
    "\n",
    "beer_onion_garlic = sum(\n",
    "    1 for txt in beer_texts if re.search(onion_garlic_pattern, txt, flags=re.I)\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"OG descriptor analysis:\")\n",
    "\n",
    "print(\n",
    "    f\"\\nDried hops comments with onion/garlic: {hops_onion_garlic}/{len(hops_texts)} ({hops_onion_garlic / len(hops_texts) * 100:.1f}%)\"\n",
    ")\n",
    "print(\n",
    "    f\"Beer comments with onion/garlic: {beer_onion_garlic}/{len(beer_texts)} ({beer_onion_garlic / len(beer_texts) * 100:.1f}%)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical descriptor analysis\n",
    "Define different descriptor categories (tropical fruits, citrus fruits, etc.) and analyze their prevalence across all comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical descriptor analysis:\n",
      "\n",
      "Dried hops\n",
      "Rank Category           Count   Percentage\n",
      "---------------------------------------------\n",
      "1    tropical fruits    48         26.5%\n",
      "2    citrus fruits      45         24.9%\n",
      "3    fruit (general)    39         21.5%\n",
      "4    stone fruits       23         12.7%\n",
      "5    onion garlic       18          9.9%\n",
      "6    melons             8           4.4%\n",
      "7    berries            7           3.9%\n",
      "\n",
      "Beer\n",
      "Rank Category           Count   Percentage\n",
      "---------------------------------------------\n",
      "1    tropical fruits    21         24.1%\n",
      "2    fruit (general)    21         24.1%\n",
      "3    citrus fruits      18         20.7%\n",
      "4    stone fruits       9          10.3%\n",
      "5    onion garlic       5           5.7%\n",
      "6    melons             5           5.7%\n",
      "7    berries            0           0.0%\n"
     ]
    }
   ],
   "source": [
    "# Define descriptor categories with regex patterns\n",
    "descriptor_categories = {\n",
    "    \"tropical fruits\": r\"\\b(tropical|pineapple|mango|passion|papaya|guava|lychee)\\b\",\n",
    "    \"citrus fruits\": r\"\\b(citrus|orange|grapefruit|lemon|lime|tangerine|mandarin|yuzu|tangelo)\\b\",\n",
    "    \"stone fruits\": r\"\\b(stone fruit|peach|nectarine|apricot|plum)\\b\",\n",
    "    \"fruit (general)\": r\"\\b(fruit|fruity)\\b\",\n",
    "    \"onion garlic\": r\"\\b(onion|garlic)\\b\",\n",
    "    \"melons\": r\"\\b(melon|watermelon|cantaloupe|honeydew)\\b\",\n",
    "    \"berries\": r\"\\b(berry|strawberry|blackberry|blueberry|raspberry)\\b\",\n",
    "}\n",
    "\n",
    "# Calculate prevalence for dried hops\n",
    "hops_total = len(hops_texts)\n",
    "hops_rows = []\n",
    "for label, pattern in descriptor_categories.items():\n",
    "    n = sum(bool(re.search(pattern, text, flags=re.I)) for text in hops_texts)\n",
    "    hops_rows.append(\n",
    "        {\n",
    "            \"category\": label,\n",
    "            \"n_comments\": n,\n",
    "            \"pct_comments\": n / hops_total,\n",
    "            \"n_total\": hops_total,\n",
    "        }\n",
    "    )\n",
    "hops_prevalence = pd.DataFrame(hops_rows).sort_values(\"pct_comments\", ascending=False)\n",
    "\n",
    "# Calculate prevalence for beer\n",
    "beer_total = len(beer_texts)\n",
    "beer_rows = []\n",
    "for label, pattern in descriptor_categories.items():\n",
    "    n = sum(bool(re.search(pattern, text, flags=re.I)) for text in beer_texts)\n",
    "    beer_rows.append(\n",
    "        {\n",
    "            \"category\": label,\n",
    "            \"n_comments\": n,\n",
    "            \"pct_comments\": n / beer_total,\n",
    "            \"n_total\": beer_total,\n",
    "        }\n",
    "    )\n",
    "beer_prevalence = pd.DataFrame(beer_rows).sort_values(\"pct_comments\", ascending=False)\n",
    "\n",
    "# Display results\n",
    "print(\"Categorical descriptor analysis:\")\n",
    "\n",
    "print(f\"\\nDried hops\")\n",
    "print(f\"{'Rank':<4} {'Category':<18} {'Count':<7} {'Percentage':<10}\")\n",
    "print(\"-\" * 45)\n",
    "for i, row in hops_prevalence.iterrows():\n",
    "    rank = hops_prevalence.index.get_loc(i) + 1\n",
    "    print(\n",
    "        f\"{rank:<4} {row['category']:<18} {row['n_comments']:<7} {row['pct_comments']*100:>7.1f}%\"\n",
    "    )\n",
    "\n",
    "print(f\"\\nBeer\")\n",
    "print(f\"{'Rank':<4} {'Category':<18} {'Count':<7} {'Percentage':<10}\")\n",
    "print(\"-\" * 45)\n",
    "for i, row in beer_prevalence.iterrows():\n",
    "    rank = beer_prevalence.index.get_loc(i) + 1\n",
    "    print(\n",
    "        f\"{rank:<4} {row['category']:<18} {row['n_comments']:<7} {row['pct_comments']*100:>7.1f}%\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
