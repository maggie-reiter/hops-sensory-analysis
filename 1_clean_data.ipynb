{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning and preprocessing\n",
    "This notebook performs data preprocessing on sensory comments from hop and beer evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package versions:\n",
      "Python: 3.12.7 (tags/v3.12.7:0b05ead, Oct  1 2024, 03:06:41) [MSC v.1941 64 bit (AMD64)]\n",
      "pandas: 2.2.3\n",
      "numpy: 1.26.4\n",
      "nltk: 3.9.1\n"
     ]
    }
   ],
   "source": [
    "# Import required packages\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Package versions:\")\n",
    "print(f\"Python: {__import__('sys').version}\")\n",
    "print(f\"pandas: {pd.__version__}\")\n",
    "print(f\"numpy: {np.__version__}\")\n",
    "print(f\"nltk: {nltk.__version__}\")\n",
    "\n",
    "# Download required NLTK data if not already present\n",
    "try:\n",
    "    nltk.data.find(\"tokenizers/punkt\")\n",
    "except LookupError:\n",
    "    print(\"Downloading NLTK punkt tokenizer...\")\n",
    "    nltk.download(\"punkt\", quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "Load the raw sensory comments data from the CSV file containing evaluations from multiple tasting events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (268, 3)\n",
      "Columns: ['Event', 'Sample', 'Comments']\n",
      "\n",
      "Sample type distribution:\n",
      "Sample\n",
      "Dried Hops    181\n",
      "Beer           87\n",
      "Name: count, dtype: int64\n",
      "\n",
      "First 5 rows:\n",
      "                         Event      Sample           Comments\n",
      "0  Hopsource 2022 - Washington  Dried Hops  Complex and clean\n",
      "1  Hopsource 2022 - Washington  Dried Hops       Vinous grape\n",
      "2  Hopsource 2022 - Washington  Dried Hops        red flowers\n",
      "3  Hopsource 2022 - Washington  Dried Hops              Muted\n",
      "4  Hopsource 2022 - Washington  Dried Hops        Some fruity\n"
     ]
    }
   ],
   "source": [
    "# Load the raw sensory comments data\n",
    "data = pd.read_csv(\"Vera Sensory Comments.csv\")\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(f\"Dataset shape: {data.shape}\")\n",
    "print(f\"Columns: {list(data.columns)}\")\n",
    "\n",
    "# Show the distribution of sample types\n",
    "print(f\"\\nSample type distribution:\")\n",
    "print(data[\"Sample\"].value_counts())\n",
    "\n",
    "# Display the first few rows to understand the data structure\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expand acronyms\n",
    "Identify acronyms in the comments and expand them to their full forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identified 8 unique acronyms:\n",
      "['DMTS', 'GO', 'IPA', 'IPL', 'O/G', 'ONG', 'T2N', 'WOW']\n"
     ]
    }
   ],
   "source": [
    "# Define regex pattern to identify potential acronyms\n",
    "pattern = r\"\\b(?:[A-Z]+(?:/[A-Z]+)*|[A-Z]+\\d+[A-Z]*)\\b\"\n",
    "\n",
    "# Extract all unique acronyms from the comments\n",
    "acronyms = set()\n",
    "for comment in data[\"Comments\"]:\n",
    "    if pd.notna(comment):\n",
    "        matches = re.findall(pattern, str(comment))\n",
    "        for match in matches:\n",
    "            if len(match) > 1 or \"/\" in match or re.search(r\"\\d\", match):\n",
    "                acronyms.add(match)\n",
    "\n",
    "print(f\"Identified {len(acronyms)} unique acronyms:\")\n",
    "print(sorted(acronyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define mappings for domain-specific acronyms to their full forms\n",
    "# WOW is the word 'wow' in all caps\n",
    "acronym_map = {\n",
    "    \"GO\": \"onion garlic\",\n",
    "    \"O/G\": \"onion garlic\",\n",
    "    \"ONG\": \"onion garlic\",\n",
    "    \"DMTS\": \"dimethyl trisulfide\",\n",
    "    \"T2N\": \"trans-2-nonenal\",\n",
    "    \"H2S\": \"hydrogen sulfide\",\n",
    "    \"IPA\": \"india pale ale\",\n",
    "    \"IPL\": \"india pale lager\",\n",
    "}\n",
    "\n",
    "# Apply each acronym replacement\n",
    "for abbr, full in acronym_map.items():\n",
    "    pattern = rf\"\\b{re.escape(abbr)}\\b\"\n",
    "    data[\"Comments\"] = data[\"Comments\"].str.replace(\n",
    "        pattern, full, regex=True, case=False\n",
    "    )\n",
    "\n",
    "# Handle special cases\n",
    "# Plural forms of IPA\n",
    "ipa_pattern = r\"\\bindia pale ale(s?)\\b\"\n",
    "plural_count = (\n",
    "    data[\"Comments\"].str.count(r\"\\bindia pale ales\\b\", flags=re.IGNORECASE).sum()\n",
    ")\n",
    "\n",
    "# 'ohai' (Overall hop aroma intensity)\n",
    "ohai_count = data[\"Comments\"].str.count(r\"\\bohai\\b\", flags=re.IGNORECASE).sum()\n",
    "if ohai_count > 0:\n",
    "    data[\"Comments\"] = data[\"Comments\"].str.replace(\n",
    "        r\"\\bohai\\b\", \"overall hop aroma intensity\", case=False, regex=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape data\n",
    "Transform the tabular data into a structured JSON format suitable for downstream text analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 5 records:\n",
      "[\n",
      "  {\n",
      "    \"sample\": \"Dried Hops\",\n",
      "    \"text\": \"Complex and clean\"\n",
      "  },\n",
      "  {\n",
      "    \"sample\": \"Dried Hops\",\n",
      "    \"text\": \"Vinous grape\"\n",
      "  },\n",
      "  {\n",
      "    \"sample\": \"Dried Hops\",\n",
      "    \"text\": \"red flowers\"\n",
      "  },\n",
      "  {\n",
      "    \"sample\": \"Dried Hops\",\n",
      "    \"text\": \"Muted\"\n",
      "  },\n",
      "  {\n",
      "    \"sample\": \"Dried Hops\",\n",
      "    \"text\": \"Some fruity\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Extract relevant columns and rename for consistency\n",
    "records = (\n",
    "    data[[\"Sample\", \"Comments\"]]\n",
    "    .rename(columns={\"Sample\": \"sample\", \"Comments\": \"text\"})\n",
    "    .to_dict(orient=\"records\")\n",
    ")\n",
    "\n",
    "print(f\"\\nFirst 5 records:\")\n",
    "print(json.dumps(records[:5], indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count words\n",
    "Look at summary statistics to understand the distribution of text length across different sample types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of hop comments: 181\n",
      "Count of beer comments: 87\n",
      "\n",
      "Example hop comments:\n",
      "  1. \"Complex and clean\"\n",
      "  2. \"Vinous grape\"\n",
      "  3. \"red flowers\"\n",
      "\n",
      "Example beer comments:\n",
      "  1. \"Good\"\n",
      "  2. \"Woah. Interesting. Solvent notes up front - not acetone like Sabro, but not toluene or methanol either. Orange terpenes/concentrate, green onion, and non-specific tropical esters\"\n",
      "  3. \"Weird aroma\"\n"
     ]
    }
   ],
   "source": [
    "# Separate comments by sample type\n",
    "hops_texts = [item[\"text\"] for item in records if item[\"sample\"] == \"Dried Hops\"]\n",
    "beer_texts = [item[\"text\"] for item in records if item[\"sample\"] == \"Beer\"]\n",
    "\n",
    "print(f\"Count of hop comments: {len(hops_texts)}\")\n",
    "print(f\"Count of beer comments: {len(beer_texts)}\")\n",
    "\n",
    "# Show some example comments from each group\n",
    "print(f\"\\nExample hop comments:\")\n",
    "for i, text in enumerate(hops_texts[:3]):\n",
    "    print(f'  {i+1}. \"{text}\"')\n",
    "\n",
    "print(f\"\\nExample beer comments:\")\n",
    "for i, text in enumerate(beer_texts[:3]):\n",
    "    print(f'  {i+1}. \"{text}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dried hops comments (n=181):\n",
      "  Mean:           5.76 words\n",
      "  Median:         4.0 words\n",
      "  Standard dev:   5.68 words\n",
      "  Min-max:        1-41 words\n",
      "  25th-75th %ile: 2-7 words\n",
      "  10th-90th %ile: 1-13 words\n",
      "\n",
      "Beer comments (n=87):\n",
      "  Mean:           10.74 words\n",
      "  Median:         8.0 words\n",
      "  Standard dev:   10.30 words\n",
      "  Min-max:        1-43 words\n",
      "  25th-75th %ile: 3-14 words\n",
      "  10th-90th %ile: 1-27 words\n"
     ]
    }
   ],
   "source": [
    "# Define function to count words using NLTK tokenizer\n",
    "def word_count(text):\n",
    "    \"\"\"Count words in text using NLTK word tokenizer\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return 0\n",
    "    return len(word_tokenize(str(text)))\n",
    "\n",
    "\n",
    "# Calculate word counts for each sample type\n",
    "hops_word_counts = [word_count(text) for text in hops_texts]\n",
    "beer_word_counts = [word_count(text) for text in beer_texts]\n",
    "\n",
    "# Calculate percentiles for distribution analysis\n",
    "p10_hops, p25_hops, p75_hops, p90_hops = np.percentile(\n",
    "    hops_word_counts, [10, 25, 75, 90]\n",
    ")\n",
    "p10_beer, p25_beer, p75_beer, p90_beer = np.percentile(\n",
    "    beer_word_counts, [10, 25, 75, 90]\n",
    ")\n",
    "\n",
    "print(f\"\\nDried hops comments (n={len(hops_texts)}):\")\n",
    "print(f\"  Mean:           {np.mean(hops_word_counts):.2f} words\")\n",
    "print(f\"  Median:         {np.median(hops_word_counts):.1f} words\")\n",
    "print(f\"  Standard dev:   {np.std(hops_word_counts):.2f} words\")\n",
    "print(f\"  Min-max:        {np.min(hops_word_counts)}-{np.max(hops_word_counts)} words\")\n",
    "print(f\"  25th-75th %ile: {p25_hops:.0f}-{p75_hops:.0f} words\")\n",
    "print(f\"  10th-90th %ile: {p10_hops:.0f}-{p90_hops:.0f} words\")\n",
    "\n",
    "print(f\"\\nBeer comments (n={len(beer_texts)}):\")\n",
    "print(f\"  Mean:           {np.mean(beer_word_counts):.2f} words\")\n",
    "print(f\"  Median:         {np.median(beer_word_counts):.1f} words\")\n",
    "print(f\"  Standard dev:   {np.std(beer_word_counts):.2f} words\")\n",
    "print(f\"  Min-max:        {np.min(beer_word_counts)}-{np.max(beer_word_counts)} words\")\n",
    "print(f\"  25th-75th %ile: {p25_beer:.0f}-{p75_beer:.0f} words\")\n",
    "print(f\"  10th-90th %ile: {p10_beer:.0f}-{p90_beer:.0f} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save cleaned data\n",
    "Export the cleaned and standardized comments to a JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully saved to 'comments_cleaned.json'\n",
      "File size: 28,011 bytes (27.4 KB)\n"
     ]
    }
   ],
   "source": [
    "# Save the cleaned and reshaped data to JSON\n",
    "output_filename = \"comments_cleaned.json\"\n",
    "\n",
    "with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(records, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"Data successfully saved to '{output_filename}'\")\n",
    "\n",
    "# Verify the saved file\n",
    "import os\n",
    "\n",
    "file_size = os.path.getsize(output_filename)\n",
    "print(f\"File size: {file_size:,} bytes ({file_size/1024:.1f} KB)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
